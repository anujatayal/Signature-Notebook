{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import functools\n",
    "#from tqdm import tqdm\n",
    "from typing import Tuple, Callable, Dict, Union\n",
    "import numpy as np\n",
    "#from skimage import filters, transform\n",
    "#from typing import Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Iterable, Tuple\n",
    "#import numpy as np\n",
    "#import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from collections import OrderedDict\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.losses import categorical_crossentropy,binary_crossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from fr_utils import *\n",
    "#from inception_blocks_v2 import *\n",
    "# import sys\n",
    "# import glob\n",
    "# import logging\n",
    "# #from keras.models import Sequential, Model\n",
    "# import random\n",
    "# from skimage.io import imread\n",
    "# from skimage import img_as_ubyte\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableDataset(ABC):\n",
    "    @property\n",
    "    def maxsize(self):\n",
    "        raise NotImplementedError\n",
    "    @property\n",
    "    def genuine_per_user(self):\n",
    "        raise NotImplementedError\n",
    "    @property\n",
    "    def skilled_per_user(self):\n",
    "        raise NotImplementedError\n",
    "    @property\n",
    "    \n",
    "    def simple_per_user(self):\n",
    "        raise NotImplementedError\n",
    "    @abstractmethod\n",
    "    def get_user_list(self) -> List[int]:\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def iter_genuine(self, user: int) -> Iterable[Tuple[np.ndarray, str]]:\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def iter_simple_forgery(self, user: int) -> Iterable[Tuple[np.ndarray, str]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def iter_forgery(self, user: int) -> Iterable[Tuple[np.ndarray, str]]:\n",
    "        pass\n",
    "# tf.enable_eager_execution()\n",
    "class CedarDataset(IterableDataset):\n",
    "    \"\"\" Helper class to load the CEDAR dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.users = list(range(1, 55+1))\n",
    "\n",
    "    @property\n",
    "    def genuine_per_user(self):\n",
    "        return 24\n",
    "\n",
    "    @property\n",
    "    def skilled_per_user(self):\n",
    "        return 24\n",
    "\n",
    "    @property\n",
    "    def simple_per_user(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def maxsize(self):\n",
    "        return 730, 1042\n",
    "\n",
    "    def get_user_list(self):\n",
    "        return self.users\n",
    "\n",
    "    def iter_genuine(self, user):\n",
    "        \"\"\" Iterate over genuine signatures for the given user\"\"\"\n",
    "\n",
    "        files = ['{}_{}_{}.png'.format('original', user, img) for img in range(1, 24 + 1)]\n",
    "        for f in files:\n",
    "            full_path = os.path.join(self.path, 'full_org', f)\n",
    "            img = imread(full_path, as_gray=True)\n",
    "            yield img_as_ubyte(img), f\n",
    "\n",
    "    def iter_forgery(self, user):\n",
    "        \"\"\" Iterate over skilled forgeries for the given user\"\"\"\n",
    "\n",
    "        files = ['{}_{}_{}.png'.format('forgeries', user, img) for img in range(1, 24 + 1)]\n",
    "        for f in files:\n",
    "            full_path = os.path.join(self.path, 'full_forg', f)\n",
    "            img = imread(full_path, as_gray=True)\n",
    "            yield img_as_ubyte(img), f\n",
    "\n",
    "    def iter_simple_forgery(self, user):\n",
    "        yield from ()  # No simple forgeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"/home/anuja/Downloads/signature_dataset/signatures/\"\n",
    "#npzpath=\"/home/anuja/Downloads/signature_dataset/signatures/signature.npz\"\n",
    "img_height = 150\n",
    "img_width = 220\n",
    "featurewise_std_normalization = True\n",
    "batch_size = 5    \n",
    "epochs=20\n",
    "seed=42\n",
    "input_shape=(img_height, img_width,1)\n",
    "lamb = 0.95\n",
    "#checkpoint=#pth_file\n",
    "model=CedarDataset\n",
    "users=55\n",
    "#checkpointer = ModelCheckpoint(filepath=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\",verbose=1, save_best_only=False,save_weights_only=False,save_freq=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\",monitor='val_loss', mode='min',verbose=1, save_best_only=True,save_weights_only=False)\n",
    "#earlystoppetrain_file=\"/home/anuja/Downloads/signature_dataset/signatures/train_signature.npz\"\n",
    "test_file=\"/home/anuja/Downloads/signature_dataset/signatures/test_signature.npz\"\n",
    "#r = EarlyStopping(monitor='val_loss', min_delta=0,patience=4, verbose=1, mode='auto')\n",
    "lr=0.001\n",
    "lr_decay_multiplier=0.1\n",
    "lr_decay_times=0.1\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "epochs=20\n",
    "seed=42\n",
    "loss_type=\"L2\"\n",
    "forg=True\n",
    "test=False #if true then it will test after loading the model \n",
    "input_size=(150,220)\n",
    "#lamb in {0.95,0.99,0.999}\n",
    "#checkpoint=#pth_file\n",
    "lamb = 0.95\n",
    "#checkpoint=#pth_file\n",
    "model=CedarDataset\n",
    "users=55\n",
    "#no_images=10\n",
    "n_classes=55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/home/anuja/Downloads/signature_dataset/signatures.csv')\n",
    "#data=data.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('triplet_file_random.csv')\n",
    "#data=data.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')\n",
    "def train_model(input_shape):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', name='conv1_1', strides=(1,1), input_shape= input_shape, \n",
    "                    kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', name='conv1_2', strides=(1,1),kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', name='conv1_3', strides=(1,1),kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', name='conv1_4', strides=(2,2),kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', name='conv1_5', strides=(2,2),kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    #model.add(ZeroPadding2D((2, 2),))\n",
    "    model.add(Conv2D(256, (5, 5), activation='relu', name='conv2_1', strides=(1, 1),padding=\"same\",kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    #model.add(Dropout(0.3))# added extra\n",
    "    #model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(384, (3, 3), activation='relu', name='conv3_1', strides=(1, 1),padding=\"same\", kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    model.add(Conv2D(384, (3, 3), activation='relu', name='conv3_2', strides=(1, 1),padding=\"same\", kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    #model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_3', strides=(1, 1),padding=\"same\", kernel_initializer='glorot_uniform'))   \n",
    "    model.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    #model.add(Dropout(0.3))# added extra\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(128, kernel_regularizer=l2(0.0005), activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, kernel_regularizer=l2(0.0005), activation='relu', kernel_initializer='glorot_uniform')) # softmax changed to relu\n",
    "    #last_features=BatchNormalization(epsilon=1e-06, axis=-1, momentum=0.9)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(shape=input_shape)\n",
    "input_positive = Input(shape=input_shape)\n",
    "input_negative = Input(shape=input_shape)\n",
    "Shared_DNN = train_model([150,220,1,])\n",
    "encoded_image=Shared_DNN(input_image)\n",
    "encoded_positive=Shared_DNN(input_positive)\n",
    "encoded_negative=Shared_DNN(input_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "merged_vector = concatenate([encoded_image, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "model = Model(inputs=[input_image,input_positive, input_negative], outputs=merged_vector)\n",
    "data.insert(2, \"Y\", 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dummy = np.ones((int(len(data)*.9),1))\n",
    "Y_val = np.ones((int(len(data)*.1),1))\n",
    "Y_dummy.shape\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):  \n",
    "    anchor, positive, negative = y_pred[:,0], y_pred[:,1], y_pred[:,2]\n",
    "    print(anchor, positive, negative)\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive))\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative))\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = tf.maximum(basic_loss, 0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shared_DNN.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(lr=0.0001,amsgrad=True)\n",
    "model.compile(loss=triplet_loss, optimizer=adam_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img, img_to_array\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,oom_range=0.2,horizontal_flip=True)\n",
    "train_datagen = ImageDataGenerator(rotation_range=20,zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,horizontal_flip=True,fill_mode=\"nearest\")\n",
    "def gen_flow_for_three_inputs(train_datagen,df1,df2,df3,y,a,b):\n",
    "    train_anchor=train_datagen.flow_from_dataframe(dataframe=data[a:b],x_col=df1,y_col=y, color_mode='grayscale',batch_size=5,seed=42,shuffle=True,class_mode=\"other\",target_size=input_size)\n",
    "    train_positive=train_datagen.flow_from_dataframe(dataframe=data[a:b],x_col=df2,y_col=y, color_mode='grayscale',batch_size=5,seed=42,shuffle=True,class_mode=\"other\",target_size=input_size)\n",
    "    train_negative=train_datagen.flow_from_dataframe(dataframe=data[a:b],x_col=df3,y_col=y, color_mode='grayscale',batch_size=5,seed=42,shuffle=True,class_mode=\"other\",target_size=input_size)\n",
    "    while True:\n",
    "            anchor = train_anchor.next()\n",
    "            positive = train_positive.next()\n",
    "            negative=train_negative.next()\n",
    "            yield [anchor[0], positive[0],negative[0]], anchor[1]\n",
    "input_gen=gen_flow_for_three_inputs(train_datagen,\"Anchor\",\"Positive\",\"Negative\",\"Y\",0,len(Y_dummy))\n",
    "test_gen=gen_flow_for_three_inputs(train_datagen,\"Anchor\",\"Positive\",\"Negative\",\"Y\",len(Y_dummy),len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "model.fit_generator(generator=input_gen,validation_data=test_gen,validation_steps=len(Y_val)/batch_size,epochs=2,shuffle=True,steps_per_epoch=len(Y_dummy)/batch_size,verbose=1,callbacks=[checkpointer])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
